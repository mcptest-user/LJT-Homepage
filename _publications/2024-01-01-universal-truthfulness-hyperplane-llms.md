

---
title: "On the Universal Truthfulness Hyperplane Inside LLMs"
collection: publications
category: conferences
permalink: /publication/2024-01-01-universal-truthfulness-hyperplane-llms
excerpt: 'This paper investigates the existence of a universal truthfulness hyperplane in large language models that can be used to control truthfulness across different domains.'
date: 2024-01-01
venue: 'Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP)'
paperurl: 'https://aclanthology.org/2024.emnlp-main.xxx/'
citation: 'Junteng Liu, Shiqi Chen, Yu Cheng, Junxian He. (2024). &quot;On the Universal Truthfulness Hyperplane Inside LLMs.&quot; In <i>Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP)</i>.'
---
This work explores the internal representations of large language models to identify a universal truthfulness hyperplane that can be used to control the truthfulness of model outputs across different domains. We demonstrate that such a hyperplane exists and can be leveraged to improve model reliability and reduce hallucinations.

[GitHub Repository: Universal_Truthfulness_Hyperplane](https://github.com/Vicent0205/Universal_Truthfulness_Hyperplane)

