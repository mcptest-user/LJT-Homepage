


---
title: "In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation"
collection: publications
category: conferences
permalink: /publication/2024-01-01-in-context-sharpness-alerts-hallucination-mitigation
excerpt: 'This paper proposes using in-context sharpness as alerts from an inner representation perspective to mitigate hallucinations in language models.'
date: 2024-01-01
venue: 'Proceedings of the 41st International Conference on Machine Learning (ICML)'
paperurl: 'https://proceedings.mlr.press/v235/chen24a.html'
citation: 'Shiqi Chen, Miao Xiong, Junteng Liu, Zhengxuan Wu, Teng Xiao, Siyang Gao, Junxian He. (2024). &quot;In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation.&quot; In <i>Proceedings of the 41st International Conference on Machine Learning (ICML)</i>.'
---
This work introduces a novel approach to hallucination mitigation in language models by leveraging in-context sharpness as alerts from an inner representation perspective. We demonstrate that sharpness in contextual representations can serve as effective indicators for potential hallucinations, enabling more reliable model outputs across various tasks and domains.

